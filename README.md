# magoya-exercise
Desafío Técnico: DevOps - SRE

## Challenge
See the definition at [full description file](Challenge%20DevOps.pdf)

## Structure Definition
The solution held in this repository has been divided into 3 areas/folder.

### App folder
A simple react app that contains a random image that can be updated by clicking a button. The goal of this action is to generate traffic that can be captured later on by Grafana and create dashboard of it.
It contains a [dockerfile](app/simple-homepage/Dockerfile) that is used to push a docker image of the application so it can be pull from a deployment into a k8s cluster

### Kubernetes folder
Keeps the objects to be deployed in the kubernetes cluster. The deployment could be done by using [kubectl](kubernetes/kubectl) or using [terraform](kubernetes/terraform).
In case you want to use kubectl, all the objects can be automatically created by running **install_k8s.sh** file, bear in mind the script assumes you have configured kubectl correctly
As well as Terraform, the deployment can be done by running the **terraform apply** on the whole folder, previously configured

### Terraform folder
This is the whole infrastructure definition files. It creates a cluster in AWS, creates iam roles to handle the cluster and the nodes and make everithing connected trought a VPC.
The [main](terraform/main.tf) file uses the modules defined inside module folder but also extends to an s3 bucket where the terraform state is saved.
The module folder saves the VPC, EKS and IAM configuration to create all the needed objects in AWS

- The VPC folder creates a virtual conection for all the objects. First it sets two kind of subnet: a Public and a Private one. Each of it has two instances asociated to the availability zones.
Every subnet has a large set of IP so we will be ok if we consider that __10.0.0.0 / 24__ gives you about 65.536 subnets and 254 hosts per subred

- The IAM folder generates the roles that would be atach to the modules so the access tp them is configured directly in this folder. Once the role is created it get assigned to the nodes and the cluster, as well as the ec2 instances autogenerated by the cluser configuration

- The EKS folder contains the cluster definitios as it stands. It generates the cluster within two nodes. The idea is to have a node specific for the kubectl system configuration and any other needed tool to be installed (i.e. a monitoring tool). The aplication node will be used to deploy the aplication image(s) as well as databeses conections, pvc or any other k8s object

## Execute the code

1. Configure Terraform with AWS credentials

    Use export env variables

    ```
    export AWS_ACCESS_KEY_ID=

    export AWS_SECRET_ACCESS_KEY=
    ```

2. Navigate to the [terraform folder](terraform)
    - Run terraform basic commands to deploy the infra
    ```
    terraform init
    terraform plan
    terraform apply
    ```

4. Once the process is complete you can check cluster status by running some kubectl commands
    - Update your context
        ```
        aws eks --region us-west-1 update-kubeconfig --name magoya-testing
        ```
    - Check cluster status
        ```
        kubectl cluster-info
        kubectl get ns
        kubectl get nods
        .....
        ```
5. Deploy the kubernetes objects
    . As explained above either use terraform, or use kubectl with the *install_k8s.sh* file

6. TODO: Connect to the app deployed
